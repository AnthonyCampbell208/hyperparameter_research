{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import (BaseCrossValidator, GridSearchCV, KFold,\n",
    "                                     RandomizedSearchCV, StratifiedKFold,\n",
    "                                     check_cv, train_test_split)\n",
    "from sklearn.linear_model import (ARDRegression, BayesianRidge, ElasticNet,\n",
    "                                  ElasticNetCV, Lars, Lasso, LassoLars,\n",
    "                                  LinearRegression, LogisticRegression,\n",
    "                                  LogisticRegressionCV,\n",
    "                                  OrthogonalMatchingPursuit, Ridge)\n",
    "from sklearn.ensemble import (GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.base import BaseEstimator, is_regressor\n",
    "# import econml\n",
    "# from econml.orf import DMLOrthoForest\n",
    "# from econml.metalearners import SLearner, TLearner, XLearner\n",
    "# from econml.grf import CausalForest\n",
    "from econml.dr import DRLearner\n",
    "from econml.dml import CausalForestDML, KernelDML, LinearDML, SparseLinearDML\n",
    "import sklearn.preprocessing\n",
    "import sklearn.neural_network\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import utils\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Cross-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_classification_hyperparameters(estimator):\n",
    "    \"\"\"\n",
    "    Returns a hyperparameter grid for the specified classification model type.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): The type of model to be used. Valid values are 'linear', 'forest', 'nnet', and 'poly'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the hyperparameter grid to search over.\n",
    "    \"\"\"\n",
    "    if isinstance(estimator, LogisticRegressionCV) or estimator == 'linear':\n",
    "        # Hyperparameter grid for linear classification model\n",
    "        return {\n",
    "            'Cs': [1, 10],\n",
    "            'max_iter': [25]\n",
    "        }\n",
    "    elif isinstance(estimator, RandomForestClassifier) or estimator == 'forest':\n",
    "        # Hyperparameter grid for random forest classification model\n",
    "        return {\n",
    "            'n_estimators': [25],\n",
    "            'max_depth': [None, 5, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    elif isinstance(estimator, GradientBoostingClassifier) or estimator == 'gbf':\n",
    "        # Hyperparameter grid for gradient boosting classification model\n",
    "        return {\n",
    "            'n_estimators': [100, 500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "\n",
    "        }\n",
    "    elif isinstance(estimator, MLPClassifier) or estimator == 'nnet':\n",
    "        # Hyperparameter grid for neural network classification model\n",
    "        return {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'activation': ['relu'],\n",
    "            'solver': ['adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'adaptive'],\n",
    "            'max_iter': [25]\n",
    "        }\n",
    "    else:\n",
    "        warnings.warn(\"No hyperparameters for this type of model. There are default hyperparameters for LogisticRegressionCV, RandomForestClassifier, MLPClassifier, and the polynomial pipleine\", category=UserWarning)\n",
    "        return {}\n",
    "        # raise ValueError(\"Invalid model type. Valid values are 'linear', 'forest', 'nnet', and 'poly'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_regression_hyperparameters(estimator):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of hyperparameters to be searched over for a regression model.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): The type of model to be used. Valid values are 'linear', 'forest', 'nnet', and 'poly'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of hyperparameters to be searched over using a grid search.\n",
    "    \"\"\"\n",
    "    if  isinstance(estimator, ElasticNetCV) or estimator == 'linear':\n",
    "        return {\n",
    "            'l1_ratio': [0.1, 0.5, 0.9],\n",
    "            'max_iter': [100],\n",
    "        }\n",
    "    elif isinstance(estimator, RandomForestRegressor) or estimator == 'forest':\n",
    "        return {\n",
    "            'n_estimators': [25],\n",
    "            'max_depth': [None, 10, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    elif isinstance(estimator, MLPRegressor) or estimator == 'nnet':\n",
    "        # Hyperparameter grid for neural network classification model\n",
    "        return {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'adaptive'],\n",
    "            'max_iter': [25]\n",
    "        }\n",
    "    elif isinstance(estimator, GradientBoostingRegressor) or estimator == 'gbf':\n",
    "        # Hyperparameter grid for gradient boosting regression model\n",
    "        return {\n",
    "            'n_estimators': [100],\n",
    "            'learning_rate': [0.01, 0.1, 1.0],\n",
    "            'max_depth': [3, 5],\n",
    "        }\n",
    "    elif isinstance(estimator, RandomForestRegressor) or estimator == 'forest':\n",
    "        return {'n_estimators': [10]}\n",
    "    else:\n",
    "        warnings.warn(\"No hyperparameters for this type of model. There are default hyperparameters for ElasticNetCV, RandomForestRegressor, MLPRegressor, and the polynomial pipeline.\", category=UserWarning)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_hyperparam(X, Y, model, is_discrete):\n",
    "    if type(model) == str:\n",
    "        if is_discrete:\n",
    "            model = utils.select_discrete_estimator(model)\n",
    "        else:\n",
    "            model = utils.select_continuous_estimator(model)\n",
    "    if is_discrete:\n",
    "        hyperparam_grid_model = select_classification_hyperparameters(model)\n",
    "    else:\n",
    "        hyperparam_grid_model = select_regression_hyperparameters(model)\n",
    "    # print(model)\n",
    "    grid_search = GridSearchCV(model, hyperparam_grid_model, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X, Y.ravel())\n",
    "    best_params = grid_search.best_params_\n",
    "    current_best_model = grid_search.best_estimator_\n",
    "    current_best_score = grid_search.best_score_\n",
    "\n",
    "    # print(current_best_model, type(current_best_model))\n",
    "    return current_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into k folds.\n",
    "#For each fold, select a different model and find the best hyperparameter \n",
    "# get score, mse, time of estimators. Set cv=0 for estimator\n",
    "def get_models_during_k_folds(X, T, Y, ci_estimator_list, model_y, model_t, model_y_discrete, model_t_discrete):\n",
    "    k = 4\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=123)\n",
    "\n",
    "    fold_models = {}\n",
    "    i = 0\n",
    "    total_start_time = time.time()\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, T_train, Y_train = X.iloc[train_index], T.iloc[train_index], Y.iloc[train_index]\n",
    "        X_test, T_test, Y_test = X.iloc[test_index], T.iloc[test_index], Y.iloc[test_index]\n",
    "        causal_model_score = {}\n",
    "        causal_model_mse = {}\n",
    "        causal_model_time = {}\n",
    "        for ci in ci_estimator_list:\n",
    "            if model_y_discrete:\n",
    "                current_model_y = utils.select_discrete_estimator(model_y[i])\n",
    "            else:\n",
    "                current_model_y = utils.select_continuous_estimator(model_y[i])\n",
    "            if model_t_discrete:\n",
    "                current_model_t = utils.select_discrete_estimator(model_t[i])\n",
    "            else:\n",
    "                current_model_t = utils.select_continuous_estimator(model_t[i])\n",
    "\n",
    "\n",
    "            #find the best hyperparameters for the first stage linear models\n",
    "            best_model_y = find_best_model_hyperparam(X, Y, current_model_y, model_y_discrete)\n",
    "            best_model_t = find_best_model_hyperparam(X, T, current_model_t, model_t_discrete)\n",
    "\n",
    "            # best_hyperparam_model_y, best_hyperparam_model_t\n",
    "            causal_model = utils.get_estimators(ci, best_model_y, best_model_t)\n",
    "            start_time = time.time()\n",
    "            causal_model.fit(Y_train, T_train, X=X_train)\n",
    "            run_time = time.time() - start_time\n",
    "            te_pred = causal_model.effect(X_test)\n",
    "            causal_model_mse[ci] = np.mean((Y_test - te_pred)**2)\n",
    "            causal_model_time[ci] = run_time\n",
    "        fold_models[f'fold {i}'] = {'model_y' : best_model_y, 'model_t' : best_model_t, 'Mse' : causal_model_mse, 'Runtime' : causal_model_time}\n",
    "        i += 1\n",
    "\n",
    "    total_run_time = time.time() - total_start_time\n",
    "    return fold_models, total_run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best fold in terms of MSE and Runtime for each estimator\n",
    "def find_best_fold_mse_runtime(fold_models, ci_estimator_list):\n",
    "    mse_all_estimators = {}\n",
    "    runtime_all_estimators = {}\n",
    "    for ci in ci_estimator_list:\n",
    "        mse_all_estimators[f'{ci}'] = []\n",
    "        runtime_all_estimators[f'{ci}'] = []\n",
    "    for k, value in fold_models.items():\n",
    "        for ci in ci_estimator_list:\n",
    "            mse_all_estimators[f'{ci}'].append(value['Mse'][ci])\n",
    "            runtime_all_estimators[f'{ci}'].append(value['Runtime'][ci])\n",
    "\n",
    "    best_mse_fold = {}\n",
    "    best_runtime_fold = {}\n",
    "    for ci in ci_estimator_list:\n",
    "        best_mse_fold[ci] = np.argmin(mse_all_estimators[f'{ci}'])\n",
    "        best_runtime_fold[ci] = np.argmin(runtime_all_estimators[f'{ci}'])\n",
    "\n",
    "    best_models_mse = {}\n",
    "    best_models_time = {}\n",
    "    for ci in ci_estimator_list:\n",
    "        fold_mse = best_mse_fold[ci]\n",
    "        fold_time = best_runtime_fold[ci]\n",
    "        best_models_mse[ci] = {'best_model_y' : fold_models[f'fold {fold_mse}']['model_y'], 'best_model_t' : fold_models[f'fold {fold_mse}']['model_t'], 'Mse' : fold_models[f'fold {fold_mse}']['Mse'][ci]}\n",
    "        best_models_time[ci] = {'best_model_y' : fold_models[f'fold {fold_time}']['model_y'], 'best_model_t' : fold_models[f'fold {fold_time}']['model_t'], 'Runtime' : fold_models[f'fold {fold_time}']['Runtime'][ci]}\n",
    "\n",
    "    return mse_all_estimators, best_mse_fold, best_models_mse, runtime_all_estimators, best_runtime_fold, best_models_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, X, T, Y, true_ITE, true_ATE, true_ATE_stderr, is_discrete = utils.load_ihdp()\n",
    "# X, X_val, T, T_val, Y, Y_val = train_test_split(X, T, Y, train_size=0.6,shuffle=True, random_state=123)\n",
    "# X_val, X_test, T_val, T_test, Y_val, Y_test = train_test_split(X_val, T_val, Y_val, train_size=.5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_estimator_list = ['tl', 'dml', 'kernel_dml', 'CausalForestDML']\n",
    "model_y = ['linear', 'forest', 'gbf', 'nnet']\n",
    "model_t = ['linear', 'forest', 'gbf', 'nnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_meta = False\n",
    "fold_models, total_run_time = get_models_during_k_folds(X, T, Y, ci_estimator_list, model_y, model_t, model_y_discrete=False, model_t_discrete=True)\n",
    "print(fold_models, total_run_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'fold 0': {'model_y': ElasticNetCV(l1_ratio=0.9, max_iter=100), 'model_t': LogisticRegressionCV(Cs=1, max_iter=25), 'Mse': {'tl': 7.522415610112231, 'dml': 6.258760351060026, 'kernel_dml': 5.314349707327539, 'CausalForestDML': 6.070481862241069}, 'Runtime': {'tl': 0.1490309238433838, 'dml': 8.739501237869263, 'kernel_dml': 0.24778056144714355, 'CausalForestDML': 0.376253604888916}}, \n",
    "\n",
    "'fold 1': {'model_y': RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators=25), 'model_t': RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=25), 'Mse': {'tl': 8.35734849455551, 'dml': 8.786954144845003, 'kernel_dml': 5.401484369803926, 'CausalForestDML': 6.038643439179678}, 'Runtime': {'tl': 0.09996724128723145, 'dml': 1.182527780532837, 'kernel_dml': 0.2831137180328369, 'CausalForestDML': 0.4049530029296875}}, \n",
    "\n",
    "'fold 2': {'model_y': GradientBoostingRegressor(), 'model_t': GradientBoostingClassifier(learning_rate=0.01), 'Mse': {'tl': 8.664446722679891, 'dml': 6.432632858973467, 'kernel_dml': 4.720149305256853, 'CausalForestDML': 5.3016284760187204}, 'Runtime': {'tl': 0.14995050430297852, 'dml': 2.421130418777466, 'kernel_dml': 0.4446289539337158, 'CausalForestDML': 0.5565671920776367}}, \n",
    "\n",
    "'fold 3': {'model_y': MLPRegressor(max_iter=25), 'model_t': MLPClassifier(hidden_layer_sizes=(50,), max_iter=25), 'Mse': {'tl': 13.107047259176383, 'dml': 9.850744661505283, 'kernel_dml': 4.446290403577857, 'CausalForestDML': 6.526513696665307}, 'Runtime': {'tl': 0.24723076820373535, 'dml': 0.7107350826263428, 'kernel_dml': 0.418302059173584, 'CausalForestDML': 0.5394127368927002}}} \n",
    "\n",
    "95.19353413581848\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_all_estimators, best_mse_fold, best_models_mse, runtime_all_estimators, best_runtime_fold, best_models_time = find_best_fold_mse_runtime(fold_models, ci_estimator_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tl': [7.522415610112231,\n",
       "   8.35734849455551,\n",
       "   8.664446722679891,\n",
       "   13.107047259176383],\n",
       "  'dml': [6.258760351060026,\n",
       "   8.786954144845003,\n",
       "   6.432632858973467,\n",
       "   9.850744661505283],\n",
       "  'kernel_dml': [5.314349707327539,\n",
       "   5.401484369803926,\n",
       "   4.720149305256853,\n",
       "   4.446290403577857],\n",
       "  'CausalForestDML': [6.070481862241069,\n",
       "   6.038643439179678,\n",
       "   5.3016284760187204,\n",
       "   6.526513696665307]},\n",
       " {'tl': 0, 'dml': 0, 'kernel_dml': 3, 'CausalForestDML': 2},\n",
       " {'tl': {'best_model_y': ElasticNetCV(l1_ratio=0.9, max_iter=100),\n",
       "   'best_model_t': LogisticRegressionCV(Cs=1, max_iter=25),\n",
       "   'Mse': 7.522415610112231},\n",
       "  'dml': {'best_model_y': ElasticNetCV(l1_ratio=0.9, max_iter=100),\n",
       "   'best_model_t': LogisticRegressionCV(Cs=1, max_iter=25),\n",
       "   'Mse': 6.258760351060026},\n",
       "  'kernel_dml': {'best_model_y': MLPRegressor(max_iter=25),\n",
       "   'best_model_t': MLPClassifier(hidden_layer_sizes=(50,), max_iter=25),\n",
       "   'Mse': 4.446290403577857},\n",
       "  'CausalForestDML': {'best_model_y': GradientBoostingRegressor(),\n",
       "   'best_model_t': GradientBoostingClassifier(learning_rate=0.01),\n",
       "   'Mse': 5.3016284760187204}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_all_estimators, best_mse_fold, best_models_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tl': [0.1490309238433838,\n",
       "   0.09996724128723145,\n",
       "   0.14995050430297852,\n",
       "   0.24723076820373535],\n",
       "  'dml': [8.739501237869263,\n",
       "   1.182527780532837,\n",
       "   2.421130418777466,\n",
       "   0.7107350826263428],\n",
       "  'kernel_dml': [0.24778056144714355,\n",
       "   0.2831137180328369,\n",
       "   0.4446289539337158,\n",
       "   0.418302059173584],\n",
       "  'CausalForestDML': [0.376253604888916,\n",
       "   0.4049530029296875,\n",
       "   0.5565671920776367,\n",
       "   0.5394127368927002]},\n",
       " {'tl': 1, 'dml': 3, 'kernel_dml': 0, 'CausalForestDML': 0},\n",
       " {'tl': {'best_model_y': RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators=25),\n",
       "   'best_model_t': RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5,\n",
       "                          n_estimators=25),\n",
       "   'Runtime': 0.09996724128723145},\n",
       "  'dml': {'best_model_y': MLPRegressor(max_iter=25),\n",
       "   'best_model_t': MLPClassifier(hidden_layer_sizes=(50,), max_iter=25),\n",
       "   'Runtime': 0.7107350826263428},\n",
       "  'kernel_dml': {'best_model_y': ElasticNetCV(l1_ratio=0.9, max_iter=100),\n",
       "   'best_model_t': LogisticRegressionCV(Cs=1, max_iter=25),\n",
       "   'Runtime': 0.24778056144714355},\n",
       "  'CausalForestDML': {'best_model_y': ElasticNetCV(l1_ratio=0.9, max_iter=100),\n",
       "   'best_model_t': LogisticRegressionCV(Cs=1, max_iter=25),\n",
       "   'Runtime': 0.376253604888916}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_all_estimators, best_runtime_fold, best_models_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
